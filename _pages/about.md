---
permalink: /
title: "Know about me !"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
{% include base_path %}

[2017 ~ Now]
> Machine/Computer Vision is such an <b>Exciting</b>/ <b>Fast-evolving</b>/ <b>Application-practical</b> field and I am delightful to be a part of it ! As a PhD. student under supervision of Professor <a href="http://www.cse.yorku.ca/~wildes/">Richard P. Wildes</a> York University Toronto Canada, my research focuses on video predictive understanding, which includes interesting tasks as Action/Activity Prediction/Anticiption, Video Prediction, Motion Planning, Vehicle/Pedestrian Trajectory tracking and etc. In a word, I do research on exploring the possible future.

[2015-2017]
> Before my Ph.D. journey, I worked with Professor <a href='https://www.bme.ufl.edu/labs/yang/'> Lin Yang</a> at University of Florida (<font color='orange'>Go</font> <font color='blue'>Gator</font> !) as a master research assitant. During my stay, we came out some really cool projects towards biomedical imaging understanding (as listed below). 

# Recent News
<div id='list_scroll'>
    <nav>
        <ul>
            <li> 2019.11.20: One anonymous submisson to <a href='http://openaccess.thecvf.com/ICCV2019.py'><b>CVPR2020</b></a></li>
            <br />
            <li> 2019.07.22: My paper "<b>Spatiotemporal Feature Residual Propagation for Action Prediction</b>" get accepted at <a href='http://openaccess.thecvf.com/ICCV2019.py'><b>ICCV2019</b></a> ! See you in Seoul ! </li>
            <br />
            <li> 2018.01.15: I become a Ph.D. student in <a href='http://vision.eecs.yorku.ca/main/'>Vision Lab</a> </li>
            <br />
            <li> 2018.01.01: I received <a href='https://gradstudies.yorku.ca/current-students/student-finances/funding-awards/ots/'>Ontario Tillium Scholarship</a> that grants 40,000\$ annually for my Ph.D. </li>
            <br />
            <li> 2018.01.01: I received <a href='https://vista.info.yorku.ca/opportunities/doctoral-scholarships/'>VISTA Scholarship</a> that grants 10,000\$ annually for my Ph.D. </li>
            <br />
            <li> 2017.06~09: I did research summer internship with <a href='http://deepinformatics.com.cn/'>Deepinformatics LLC.</a> for biomedical imaging </li>
            <br />
            <li> 2017.05.01: I graduated from University of Florida Master in Computer Science</li>
        </ul>
    </nav>
</div>
<br />

# Selected Works
<table style='background-color:transparent border-collapse:collapse'>
    <tbody>
        <tr>
            Spatiotemporal Feature Residual Propagation for Action Prediction
        </tr><br />
        <tr>
            <font color='red'>He Zhao, Richard P. Wildes</font>
        </tr>
        <tr>
            <td width="40%">
                <img src="https://JoeHEZHAO.github.io/images/iccv_2019.png">
            </td>
            <td>
                <p style="text-align: justify;">
                Recognizing actions from limited preliminary video observations
                has seen considerable recent progress. Typically,
                however, such progress has been had without explicitly
                modeling fine-grained motion evolution as a potentially
                valuable information source. We address this
                task by investigating how action patterns evolve over time in 
                a spatial feature space. There are three key components to 
                our system. First, we work with intermediate-layer ConvNet 
                features, which allow for abstraction from raw data, while 
                retaining spatial layout, which is sacrificed in approaches 
                that rely on vectorized global representations. Second, instead 
                of propagating features per se, we propagate their 
                residuals across time, which allows for a compact representation 
                that reduces redundancy while retaining essential 
                information about evolution over time. Third, we employ a 
                Kalman filter to combat error build-up and unify across prediction 
                start times.
                </p>
            </td>
        </tr>
        <tr>
            <td> </td>
            <td>
                [<a href='http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Spatiotemporal_Feature_Residual_Propagation_for_Action_Prediction_ICCV_2019_paper.pdf'>pdf</a>][<a href='JoeHEZHAO.github'>code</a>]
            </td>
        </tr>
    </tbody>
</table>
